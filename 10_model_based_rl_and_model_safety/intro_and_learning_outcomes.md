## INTRODUCTION TO THE MODULE

In the first part of this module, we will study Model-Based RL. When the agent learns a model of the environment, it can predict future states and better plan its actions. We will review the World Model concept where an AI system learns an internal, generative model of its environment.



In the second part of this module, we focus on model safety. This is particularly important in RL where a sequence of actions are taken by the agent, potentially with limited or no human oversight. We review the paper "Using Reinforcement Learning to Identify High Risk States and Treatments in Healthcare" which recognizes that RL cannot find the optimal policy for the medical application at hand. Rather, the paper provides a method for identifying and avoiding medical dead ends, which are states which invariably lead to bad outcomes.

## LEARNING OUTCOMES

At the conclusion of this module, you should be able to:

* Compare and contrast the benefits and limitations of model-free RL and model-based RL
* Outline the steps taken in the World Models paper and explore the MDN-RNN code
* Explain medical dead ends
* Discuss methods for making RL models safer 
