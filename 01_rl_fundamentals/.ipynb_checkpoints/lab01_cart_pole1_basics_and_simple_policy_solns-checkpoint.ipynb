{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GBod1nv65dN"
   },
   "source": [
    "## Lab: Cart Pole using OpenAI gym [SOLUTIONS]\n",
    "## RL Basics and Simple Policy\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: May 26, 2025\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6qBEIhrDHmX"
   },
   "source": [
    "#### Instructions:  \n",
    "\n",
    "Carefully read the notes below and run the provided code. Answer each question clearly and show all results.\n",
    "\n",
    "#### TOTAL POINTS: 10\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krpOsEbE_aTj"
   },
   "source": [
    "### Agent and Environment\n",
    "\n",
    "It is essential for the agent to have a way to get the next state and reward from the environment.\n",
    "\n",
    "Sometimes it is possible for the agent to interact with environment in real life, but often this is expensive / dangerous / impossible.\n",
    "\n",
    "We use models and simulators in this latter case.\n",
    "\n",
    "For this reason, the Gym package is useful in RL\n",
    "\n",
    "**Note**: Going forward, the fork [gymnasium](https://gymnasium.farama.org/) will maintain OpenAI gym.  \n",
    "We will be using `gymnasium` but will still refer to the environment as `gym` when it doesn't cause confusion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVwAKKjUBxFT"
   },
   "source": [
    "### Introduction to Gym\n",
    "\n",
    "Gym is a toolkit for developing and comparing RL algorithms.\n",
    "\n",
    "It comes with many pre-built environments which have functionality to emulate physical environments.\n",
    "\n",
    "\n",
    "Users can build their own custom environments. See [here](https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e#:~:text=8%20min%20read-,Create%20custom%20gym%20environments%20from%20scratch%20%E2%80%94%20A%20stock%20market%20example,Atari%20games%20to%20experiment%20with.) for example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUTU3338AvT9"
   },
   "source": [
    "### Cart Pole Problem\n",
    "\n",
    "The **CartPole** problem has a small state space and action space, so it's popular for illustrating ideas.\n",
    "\n",
    "Pole is attached to a cart on a frictionless track.\n",
    "\n",
    "Pole starts upright\n",
    "\n",
    "**Goal** is to keep pole from falling over\n",
    "\n",
    "Control system by applying **force** -1 or +1 to cart.\n",
    "\n",
    "**Reward** of +1 for each timestep the pole remains upright\n",
    "\n",
    "**Episode** ends when pole is more than 12 degrees from vertical, or cart moves more than 2.4 units from center\n",
    "\n",
    "CartPole-v0 defines *solving* as getting average reward of 195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBSmnPP8DHmY"
   },
   "source": [
    "<img src=\"./cartpole.png\" alt=\"drawing\" width=\"150\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8vJKJSXDHmY"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rA_dd0h6Eoap"
   },
   "source": [
    "### Setup and First Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgSNDjuzow2U"
   },
   "source": [
    "This notebook can be easily run on [Google Colab](https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IBP2TTMDPMA"
   },
   "outputs": [],
   "source": [
    "! pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use [renderlab](https://github.com/ryanrudes/renderlab/tree/main) to visualize *results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install renderlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed_DyvHy6pbh"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import renderlab as rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA4JgtuL5jC9"
   },
   "source": [
    "Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcGLRi6F6vcQ",
    "outputId": "853c21c3-3cd6-4226-950d-8a5198f45e09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.04225422, 0.02126478, 0.02520455, 0.00700802], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"rgb_array\")\n",
    "\n",
    "state = env.reset(seed=314)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtQFJsxj7aBj"
   },
   "source": [
    "Given the state, we take an action. The next state comes from the environment, which is encoded in `gym`.\n",
    "\n",
    "The first element holds components:   \n",
    "[0]: cart horizontal position (0.0 = center)  \n",
    "[1]: velocity (positive means right)  \n",
    "[2]: angle of the pole (0.0 = vertical)  \n",
    "[3]: pole's angular velocity (positive means clockwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7A5peNKiBXsh",
    "outputId": "59a30da4-be9d-4f8d-a178-8fd5dbf66b3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state space number of components\n",
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GkZmsIN7ky6"
   },
   "source": [
    "The action space consists of two options:\n",
    "\n",
    "[0]: move cart left   \n",
    "[1]: move cart right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySQ8EpsV7Ngr",
    "outputId": "bb78688d-b282-4338-a9ce-c229ed60906a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_68lpUuVCc-_"
   },
   "source": [
    "Let's take an action, draw a sample and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKFEIHjL7n7h",
    "outputId": "504c31ce-d526-4bfa-9842-ab4af59d7e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state [ 0.10325637  1.1913344  -0.05987212 -1.7361434 ]\n",
      "reward 1.0\n",
      "done False\n",
      "info {}\n"
     ]
    }
   ],
   "source": [
    "# move right\n",
    "action = 1\n",
    "\n",
    "# take a step and get next state, reward from environment\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "done = terminated or truncated\n",
    "\n",
    "print('state', state)\n",
    "print('reward', reward)\n",
    "print('done', done)\n",
    "print('info', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXcVDj2cDHma"
   },
   "source": [
    "**Note about DONE**  \n",
    "We need to understand if the episode is done after taking action  \n",
    "\n",
    "The API now gives more detail on this variable, which may reach `done` state for two reasons:  \n",
    "- **terminated**=True if environment terminates (eg. due to task completion, failure etc.)  \n",
    "- **truncated**=True if episode truncates due to a time limit or a reason that is not defined as part of the task MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIMLaTwvDHma"
   },
   "source": [
    "**Run several steps by taking random actions**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcXFagAwDHma",
    "outputId": "87a0cf9c-8614-48db-e88b-8e676725532f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.04267951, -0.1742094 ,  0.02534471,  0.30753553], dtype=float32), 1.0, False, False, {})\n",
      "(array([0.03919533, 0.02054241, 0.03149542, 0.02295217], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.03960617, -0.17501673,  0.03195446,  0.3254035 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.03610584, -0.37057874,  0.03846253,  0.62798977], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.02869426, -0.1760141 ,  0.05102233,  0.34766388], dtype=float32), 1.0, False, False, {})\n"
     ]
    }
   ],
   "source": [
    "state = env.reset(seed=314)\n",
    "for _ in range(5):\n",
    "    print(env.step(env.action_space.sample())) # take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run an epoch by taking random actions. Visualize the results.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rl.RenderFrame(env, \"./output\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "      break\n",
    "\n",
    "env.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqxI0vej88Kh"
   },
   "source": [
    "**Reward and Episode**  \n",
    "\n",
    "For each time step that the cart keeps the pole balanced, it earns reward 1.\n",
    "\n",
    "If the pole tilts too much or if the cart moves off screen, `reward=0` and `done=True` (the episode will end).\n",
    "\n",
    "When the episode ends, a new episode may begin. The process learns cumulatively from each episode.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv5gKVde9pvQ"
   },
   "source": [
    "#### 1) Defining a function that runs a simple policy\n",
    "**(POINTS: 1)**\n",
    "\n",
    "When the pole leans left (negative angle), move left. When the pole leans right (positive angle), move right.  \n",
    "\n",
    "The function should take the state and return an action. Test that it works properly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsAp9fdCDHma"
   },
   "outputs": [],
   "source": [
    "def simple_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cix6PfAHDHma",
    "outputId": "0226e9ec-7a66-4d96-f114-541ff6ad0296"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset(seed=314)\n",
    "simple_policy(state[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulxOXcUcDHmb"
   },
   "source": [
    "#### 2) Simulating episodes with the simple policy  \n",
    "\n",
    "2a) **(POINTS: 2)** Run 1000 episodes each with 100 time steps. Use the `simple_policy` for taking actions.  \n",
    "Each time step will call the `step()` method to get the next state and reward. Produce a boxplot of the rewards from each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pX10dPWzDHmb",
    "outputId": "1785e005-2962-4bb5-b598-fe11f4b6b617"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD4CAYAAAAjKGdbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAI80lEQVR4nO3dT4jcdxnH8c9jS9EGa1M3hkKRUJB6MNvaSlUKklSRFsrmJFgQSi69eNCDiJ5aBe96EqT+A7FSC5KlB7HULIIHJa01qTZF0FaL2iQ0WhrBg3497LQmbnD/JDuzT+b1gmF3fuzO78uT374z+2UyqTFGAOjlLbNeAACbJ94ADYk3QEPiDdCQeAM0dPV2POjCwsLYt2/fdjz0jnDu3Lns2rVr1svYUczk4sxlLTNZ69y5czl58uSZMcaejX7PtsR73759OXbs2HY89I6wsrKSAwcOzHoZO4qZXJy5rGUma62srOTgwYMvbeZ7bJsANCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3QkHgDNCTeAA2JN0BDV896AWyfG264IWfPnp3KucZD16UOvjaVc83C7t278+qrr856GfAm8b6CnT17NmOM6Zzs4XdM71wzUFWzXgJcwLYJQEPiDdCQeAM0JN4ADYk3QEPiDdDQjou3l2QBl8uV3JMdF28A1ifeAA2JN0BD4g3Q0LrxrqpvVdWpqnpuGgsC6GZxcTFV9eZtcXFx28+5kWfe30lyzzavA6ClxcXFnDhxIktLSzl9+nSWlpZy4sSJbQ/4uvEeY/wsiffCBLiIN8J95MiRLCws5MiRI28GfDtdtreEraoHkzyYJHv37s3KysqlPNZlWhWX8uewGQemeK5ZcV32NI3r8vDhwxec5/Dhw1leXt7wuV9//fXNn3SMse4tyb4kz23ka8cYueOOO8ZWrS5pZzt69Oisl7AhU53lQ9dN71wzsNVZdrlWpmmaM5nGz0CSsbS0dMGxpaWlTZ376NGjI8mxscHGjjG82gTgUuzfvz/Ly8s5dOhQzpw5k0OHDmV5eTn79+/f1vP6n3QALsHx48ezuLiY5eXl7NmzJ8lq0I8fP76t51033lX1aFa3NBeq6uUkD40xvrmtqwJoZLtDfTHrxnuMcf80FgLAxtnzBmhIvAEaEm+AhnZcvFdfNglw6a7knuy4eAOwPvEGaEi8ARoSb4CGxBugIfEGaMgbU13hpvUe1OOh667o97vevXv3rJcAFxDvK9g0X+O6srKS8fCBqZ0P5p1tE4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaEi8ARoSb4CGxBugIfEGaKjGGJf/QatOJ3npsj/wzrGQ5MysF7HDmMnFmctaZrLWQpJdY4w9G/2GbYn3la6qjo0xPjDrdewkZnJx5rKWmay1lZnYNgFoSLwBGhLvrfnGrBewA5nJxZnLWmay1qZnYs8boCHPvAEaEm+AhsR7HVX11qr6ZVX9uqp+U1Vfmhy/oaqerKrfTT7unvVap62qrqqqX1XVE5P7cz2Tqnqxqk5U1bNVdWxybN5ncn1VPV5VJ6vq+ar68DzPpKpumVwfb9xeq6rPbmUm4r2+fya5e4xxa5LbktxTVR9K8oUkT40x3pPkqcn9efOZJM+fd99MkoNjjNvOe83uvM/ka0l+PMZ4b5Jbs3q9zO1MxhgvTK6P25LckeQfSX6UrcxkjOG2wVuSa5M8k+SDSV5IcuPk+I1JXpj1+qY8i5smF9ndSZ6YHJv3mbyYZOF/js3tTJJcl+QPmbwwwkzWzOfjSX6+1Zl45r0Bk+2BZ5OcSvLkGOMXSfaOMf6SJJOP75rhEmfhq0k+n+Tf5x2b95mMJD+pqqer6sHJsXmeyc1JTif59mR77ZGq2pX5nsn5Ppnk0cnnm56JeG/AGONfY/XXnJuS3FlV75vxkmaqqu5LcmqM8fSs17LD3DXGuD3JvUk+XVUfmfWCZuzqJLcn+foY4/1JzmWOtkj+n6q6JslSkh9u9THEexPGGH9LspLkniSvVNWNSTL5eGp2K5u6u5IsVdWLSX6Q5O6q+l7meyYZY/x58vFUVvcx78x8z+TlJC9PflNNksezGvN5nskb7k3yzBjjlcn9Tc9EvNdRVXuq6vrJ529L8rEkJ5MsJ3lg8mUPJDkykwXOwBjji2OMm8YY+7L6q99PxxifyhzPpKp2VdXb3/g8q/uZz2WOZzLG+GuSP1XVLZNDH03y28zxTM5zf/67ZZJsYSb+heU6qmoxyXeTXJXVv+weG2N8uaremeSxJO9O8scknxhjvDq7lc5GVR1I8rkxxn3zPJOqujmrz7aT1e2C748xvjLPM0mSqrotySNJrkny+ySHM/k5yvzO5Nokf0py8xjj75Njm75OxBugIdsmAA2JN0BD4g3QkHgDNCTeAA2JN0BD4g3Q0H8ALKhVS1Vhp6UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_episodes = 1000\n",
    "num_steps = 100\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    ep_reward = 0\n",
    "    state = env.reset()[0]\n",
    "    for step in range(num_steps):\n",
    "        #print(state)\n",
    "        action = simple_policy(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(ep_reward)\n",
    "\n",
    "plt.boxplot(rewards, vert=False)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggnl2jKGDHmb"
   },
   "source": [
    "2b) **(POINTS: 1)** Is this policy able to solve the cart pole problem? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkZ512F-DHmb"
   },
   "source": [
    "To solve the problem, we would need an average reward of 195.0 over 100 consecutive trials.  \n",
    "Here, the reward never exceeds 70. We have not solved the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuaCVj9VDHmb"
   },
   "source": [
    "#### 3) From Question [2], what is the mean and maximum reward (roughly)?\n",
    "**(POINTS: 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHOJiByiDHmb",
    "outputId": "eacf8fbe-18e9-4fb5-dbea-4232dff91a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 41.967 max: 68.0\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "print('mean:', mean(rewards), 'max:', max(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnH4J3988wNe"
   },
   "source": [
    "#### 4) Reversed Simple Policy\n",
    "**(POINTS: 2)**\n",
    "\n",
    "What happens if you reverse the simple policy, moving left when the pole leans right, and moving right when the pole leans left? This is not a good idea, but it's instructive. To show the result, produce the boxplot from before, and calculate the mean reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_e004B5DHmb",
    "outputId": "a87f1405-389c-4200-f41c-a4ebcdd88125"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 8.745\n"
     ]
    }
   ],
   "source": [
    "def simple_policy_reversed(obs):\n",
    "    angle = obs[2]\n",
    "    return 1 if angle < 0 else 0\n",
    "\n",
    "num_episodes = 1000\n",
    "num_steps = 100\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    ep_reward = 0\n",
    "    state = env.reset()[0]\n",
    "    for step in range(num_steps):\n",
    "        #print(state)\n",
    "        action = simple_policy_reversed(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(ep_reward)\n",
    "\n",
    "plt.boxplot(rewards, vert=False)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print('mean:', mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUL-2j4HDHmb"
   },
   "source": [
    "#### 5) Modified Policy\n",
    "**(POINTS: 2)** Full points for attempt and clear explanation.\n",
    "\n",
    "Time to get creative! See if you can try a different policy that improves the mean reward. Clearly explain your strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlOzcn6sDHmb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_61H87rA_MT"
   },
   "source": [
    "---  \n",
    "\n",
    "### Wrapup\n",
    "\n",
    "This demo illustrated some basic ideas of reinforcement learning and got you started with OpenAI Gym.\n",
    "\n",
    "We will revisit this example later, bringing in more tools for a better solution.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
