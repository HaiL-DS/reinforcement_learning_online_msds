
## INTRODUCTION TO THE MODULE

We continue learning about extensions to DQN. We review Dueling Networks, which is a new architecture designed specifically for model-free RL. The dueling architecture separates state values and action advantages. Next, we study distributed deep Q-learning, which allows DQN to scale.


## LEARNING OUTCOMES

At the conclusion of this module, you should be able to:

- Explain the benefits of Dueling networks and explain how it works
- Implement a Dueling Deep Q-learning network
- Explain the Advantage function and its relationship with the Value Function and Q-Function
- Explain and differentiate the Gorila and Ape-X algorithms



